{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The goal of this project is to train a classification model that can accurately predict a samples Iron Deficiency Chlorosis (IDC) rating, given the percentages of green, yellow, and brown that exists within each height-wise segment of the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Highlights\n",
    "\n",
    "### Used Under- and Over-sampling techniques to train a classifier on an unbalanced dataset. \n",
    "\n",
    "### Each sample (data point) consists of 3 voxelated 3D point clouds that represent height-wise slices of a parent point voxelated point cloud.\n",
    "\n",
    "### Because the 3 slices are ordered (bottom to top), a reorder scheme using random selection of permutations was used to arrange the 3 vectors for each datapoint. This scheme would ensure the model is not learning to classify features based on the order of the slices but rather on the color features of the corresponding slices...REMOVING SPATIAL DEPENDENCY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 models (M0, M1, & M2) are trained. Training data consists of color percentages as features (vector [%green %yellow %brown]) and IDC ratings as labels (IDC Bins for Model M0, see below).\n",
    "\n",
    "**Although Model MB is trained, it is not used in the final hierarchical model.**\n",
    "\n",
    "### The training data for Model MB is left as-is. Features are percentages of green, yellow, and brown for each sample, and labels are the respective IDC rating for each sample.\n",
    "\n",
    "\n",
    "## <u>Model M0 Training Data</u>\n",
    "### The training data for Model M0 is first binned. \n",
    "* Samples with IDC ratings 1 & 2 belong to Bin 1. \n",
    "* Samples with IDC rating 3 belongs to Bin 2.\n",
    "* Samples with IDC ratings 4 & 5 belong to Bin 3.\n",
    "\n",
    "Therefore, the features are the color percentages, and the labels are the bin numbers.\n",
    "\n",
    "## <u>Model M1 Training Data</u>\n",
    "### The training data for Model M1 consists of only samples where the IDC rating is 1 or 2.\n",
    "The features are the color percentages, and the labels are ratings 1 or 2.\n",
    "\n",
    "\n",
    "## <u>Model M2 Training Data</u>\n",
    "### The training data for Model M2 consists of only samples where the IDC rating is 4 or 5.\n",
    "The features are the color percentages, and the labels are ratings 4 or 5.\n",
    "\n",
    "\n",
    "## The final hierarchical model uses the following logic:\n",
    "\n",
    "1. Use Model M0 to predict the bin (1, 2, or 3) the sample belongs to.\n",
    "2. If the predicted bin is 1, use Model M1 to predict an IDC rating of 1 or 2.\n",
    "3. Else if Model M0 predicts the bin to be 2, return an IDC Rating of 3.\n",
    "4. Else if Model M0 predicts the bin to be 3, use Model M2 to predict and IDC Rating of 4 or 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11.0\n",
      "0.11.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imblearn\n",
    "print(imblearn.__version__)\n",
    "#import models and performance metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn import svm\n",
    "from sklearn import svm as svm_linear\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "import sklearn\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "\n",
    "print(imblearn.__version__)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.over_sampling import SVMSMOTE\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.over_sampling import KMeansSMOTE\n",
    "\n",
    "\n",
    "import dataframe_image as dfi\n",
    "\n",
    "from itertools import permutations\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read input data\n",
    "df = pd.read_csv('3slice_colerpercentages_output.csv',index_col = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = {'percgreen':'%g','percyellow':'%y','percbrown':'%b','idc_score':'rating'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rating'] = df['rating'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will create the features array for the height-wise datasets \n",
    "#(e.g. For 3-Slice data, every three rows of color percentage (%b, %g, %y) data should represent a single datapoint\n",
    "# and one target label should be applied to that array of 3 vectors). This ensures that the three slices are treated as a single\n",
    "#datapoint\n",
    "\n",
    "def slice_feature_arrays(arg1,arg2):\n",
    "    \n",
    "    '''\n",
    "    arg1: data/array containing height-wise feature data\n",
    "    arg2: number of rows/vectors in each datapoint (number of slices)   \n",
    "    '''\n",
    "\n",
    "    #select first 3 rows (vectors) for desired fields and append to array\n",
    "    X_features = np.array([[np.array(arg1[['%b','%y','%g']][0*arg2:((0*arg2)+arg2)])]])\n",
    "    \n",
    "    #reshape features array\n",
    "    X_features = X_features.reshape(X_features.shape[0],arg2,3)\n",
    "    \n",
    "    #calculate permutation of 3 vectors (%b, %g, %y) within each array\n",
    "    feature_perm_lst_0 = list(permutations(X_features[0]))\n",
    "    \n",
    "    #use random.choice to select random permuation from list\n",
    "    X_features = np.array([random.choice(feature_perm_lst_0)])\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "    # iterate through dataframe and append desired rows (vectors) for the desired fields to current array\n",
    "    # the desired number of rows/vectors represents a single datapoint\n",
    "    # range begins at '1' since above array already contains first datapoint\n",
    "\n",
    "    for i in range(1,(int(arg1.shape[0]/arg2))):\n",
    "\n",
    "        \n",
    "        feature_ord = np.array([[np.array(arg1[['%b','%y','%g']][i*arg2:((i*arg2)+arg2)])]])\n",
    "        \n",
    "        #calculate permutation of 3 vectors (%b, %g, %y) within each array\n",
    "        feature_perm_lst = list(permutations(feature_ord[0][0]))\n",
    "        \n",
    "        #use random.choice to select random permuation from list\n",
    "        feature_unord = np.array([random.choice(feature_perm_lst)])\n",
    "        \n",
    "        #append random-ordered array to feature array\n",
    "        X_features = np.append(X_features,feature_unord,axis = 0)\n",
    "        \n",
    "        \n",
    "    return(X_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get height-wise features\n",
    "X_features = slice_feature_arrays(df,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape X features to 2D array (flatten)\n",
    "nsamples, nx, ny = X_features.shape\n",
    "X_features = X_features.reshape((nsamples,nx*ny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, inv = np.unique(df.rating, return_inverse=True)\n",
    "counts = np.bincount(inv)\n",
    "max_count = max(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training ML Classifiers with SMOTE data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a. Base Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226\n"
     ]
    }
   ],
   "source": [
    "#define features (X) and classification labels (y)\n",
    "X = X_features\n",
    "y = np.array(df['rating'][::3])\n",
    "\n",
    "u, inv = np.unique(y, return_inverse=True)\n",
    "counts = np.bincount(inv)\n",
    "max_count_base = max(counts)\n",
    "print(max_count_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((722, 9), (722,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b. Model 0 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_lst = []\n",
    "for i in range(0,df.shape[0]):\n",
    "    if df['rating'][i] == 1:\n",
    "        bin_lst.append(1)\n",
    "    elif df['rating'][i] == 2:\n",
    "        bin_lst.append(1)\n",
    "    elif df['rating'][i] == 3:\n",
    "        bin_lst.append(2)\n",
    "    elif df['rating'][i] == 4:\n",
    "        bin_lst.append(3)\n",
    "    elif df['rating'][i] == 5:\n",
    "        bin_lst.append(3)\n",
    "\n",
    "df['severity_level'] = bin_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390\n"
     ]
    }
   ],
   "source": [
    "#define features (X) and classification labels (y)\n",
    "X_m0 = X\n",
    "y_m0 = np.array(df['severity_level'][::3])\n",
    "\n",
    "u, inv = np.unique(y_m0, return_inverse=True)\n",
    "counts = np.bincount(inv)\n",
    "max_count_m0 = max(counts)\n",
    "print(max_count_m0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((722, 9), (722,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_m0.shape, y_m0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[209 123 390]\n"
     ]
    }
   ],
   "source": [
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1c. Model 1 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124\n"
     ]
    }
   ],
   "source": [
    "#model1 dataset\n",
    "X_m1 = X[((y ==1)|(y == 2))]\n",
    "y_m1 = y[((y == 1)|(y == 2))]\n",
    "\n",
    "u, inv = np.unique(y_m1, return_inverse=True)\n",
    "counts = np.bincount(inv)\n",
    "max_count_m1 = max(counts)\n",
    "print(max_count_m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((209, 9), (209,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_m1.shape, y_m1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 85 124]\n"
     ]
    }
   ],
   "source": [
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1d. Model 2 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[164 226]\n"
     ]
    }
   ],
   "source": [
    "#model2 dataset\n",
    "X_m2 = X[((y == 4)|(y == 5))]\n",
    "y_m2 = y[((y == 4)|(y == 5))]\n",
    "\n",
    "u, inv = np.unique(y_m2, return_inverse=True)\n",
    "counts = np.bincount(inv)\n",
    "max_count_m2 = max(counts)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((390, 9), (390,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_m2.shape, y_m2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def misclassification_cost(arg1, arg2):\n",
    "    \n",
    "    # function for calculating the missclassification cost of a classifier given test labels and predicted labels returned from the\n",
    "    # trained classifier.\n",
    "    \n",
    "    '''\n",
    "    inputs:\n",
    "    arg1 = array of test labels\n",
    "    arg2 = array of predicted labels\n",
    "    \n",
    "    returns:\n",
    "    misclassification cost\n",
    "    '''\n",
    "\n",
    "    #print confusion matrix\n",
    "    CM = confusion_matrix(arg1,arg2)\n",
    "    print(CM)\n",
    "\n",
    "    #define cost matrix shape\n",
    "    cM = np.zeros(CM.shape)\n",
    "\n",
    "    #assign weights to cost matrix\n",
    "    if cM.shape == (3,3):\n",
    "        cM[0] = [0,1,2]\n",
    "        cM[1] = [1,0,1]\n",
    "        cM[2] = [2,1,0]\n",
    "\n",
    "    # for binary classification\n",
    "    elif cM.shape ==(2,2):\n",
    "        cM[0] = [0,1]\n",
    "        cM[1] = [1,0]\n",
    "\n",
    "    elif cM.shape == (5,5):\n",
    "        cM[0] = [0,1,2,3,4]\n",
    "        cM[1] = [1,0,1,2,3]\n",
    "        cM[2] = [2,1,0,1,2]\n",
    "        cM[3] = [3,2,1,0,1]\n",
    "        cM[4] = [4,3,2,1,0]\n",
    "\n",
    "    #calculate classification cost\n",
    "    cM_matrix = np.matrix(CM * cM)\n",
    "    clcost = cM_matrix.sum()/arg2.shape[0]\n",
    "    \n",
    "    return(clcost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate performance of hierarchical classifier for unbinned as-is data\n",
    "\n",
    "def add_hierarchical_results(arg1, arg2, arg3):\n",
    "    \n",
    "    '''\n",
    "    This is a function that adds classification performance of the \n",
    "    hierarchical classifier to either the unbinned or binned classifier data dataframe\n",
    "    \n",
    "    arg1: array of test labels\n",
    "    arg2: array of predicted labels\n",
    "    arg3: dataframe returned from classification_pipeline0 for hierarchical classifier data to be added to\n",
    "    '''\n",
    "    \n",
    "    value = 'hierarchical'\n",
    "    if value in list(arg3['Model']):\n",
    "        print('hierarchical data already exists')\n",
    "        return(arg3)\n",
    "    else:       \n",
    "  \n",
    "\n",
    "        report_dict = classification_report(arg1,arg2,output_dict=True)\n",
    "        model = 'hierarchical'\n",
    "        accuracy = report_dict['accuracy']\n",
    "        mpca = report_dict['macro avg']['recall']\n",
    "        f1_wt = report_dict['weighted avg']['f1-score']\n",
    "        cost = misclassification_cost(arg1,arg2)\n",
    "        unique_predictions = np.unique(arg2)\n",
    "\n",
    "        #Add hierarchical classification results for as-is data to unbinned classification results for as-is data\n",
    "        table = arg3\n",
    "        table.loc[len(table.index)] = model,accuracy,mpca,f1_wt,cost,unique_predictions\n",
    "\n",
    "        return (table)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_model(arg1, arg2, arg3, arg4, arg5, arg6, arg7, arg8):\n",
    "    \n",
    "    '''\n",
    "    arg1: Input array (features) - Training data (features)\n",
    "    arg2: Target array  - Training data (labels)\n",
    "    arg3: number of items in the majority class (max count)\n",
    "    arg4: random seed for splitting data\n",
    "    arg5: oversample rate\n",
    "    arg6: undersample rate\n",
    "    arg7: undersample dictionary\n",
    "    arg8: oversample dictionary\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    SMOTE_dict = {#'SMOTE': 'sampling_strategy = over_sampling_dict,random_state = %d' % (42),\n",
    "                  #'BorderlineSMOTE': 'sampling_strategy = over_sampling_dict,random_state = %d' % (42),        \n",
    "                 'SVMSMOTE': 'sampling_strategy = over_sampling_dict,random_state = %d' % (42)}\n",
    "                 \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #create dataframe for saving results\n",
    "    output = pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    for key_1 in SMOTE_dict.keys():\n",
    "        \n",
    "        print(key_1)\n",
    "    \n",
    "    \n",
    "        #Define classifiers to use during training\n",
    "        modelDict = {#'DecisionTreeClassifier':'max_depth = %d,random_state = %d' % (4,0),\n",
    "                     'RandomForestClassifier':'',\n",
    "                     #'svm.SVC':'kernel=\"rbf\"', \n",
    "                     #'KNeighborsClassifier':'n_neighbors = 4',\n",
    "                     #'LinearDiscriminantAnalysis':'solver = \"lsqr\",shrinkage = 0.02',\n",
    "                     #'QuadraticDiscriminantAnalysis':'',\n",
    "                     #'svm_linear.SVC':'kernel=\"linear\"'\n",
    "                    }\n",
    "    \n",
    "        model_lst = list(modelDict.keys())  \n",
    "\n",
    "\n",
    "        #create lists for storing model scores for cross-validation\n",
    "        models = []\n",
    "        accuracy = []\n",
    "        mpca = []\n",
    "        f1_wt = []\n",
    "        smote = []\n",
    "\n",
    "        #create lists for storing model scores\n",
    "        accuracy2 = []\n",
    "        mpca2 = []\n",
    "        f1_wt2 = []\n",
    "        cost2 = []\n",
    "        unique2 = []\n",
    "    \n",
    "    \n",
    "    \n",
    "        for key in modelDict.keys():\n",
    "            print(key)\n",
    "            models.append(key)\n",
    "            smote.append(key_1)\n",
    "\n",
    "            #define classifier with parameters including penalizing parameters\n",
    "            clf = eval('%s(%s)' % (key,modelDict[key]))\n",
    "\n",
    "            #define over and under sampling rates\n",
    "            over_sample_rate = arg5\n",
    "            under_sample_rate = arg6\n",
    "\n",
    "\n",
    "            #assign sampling rates to dictionaries\n",
    "            under_sampling_dict = arg7\n",
    "            over_sampling_dict = arg8\n",
    "\n",
    "            # define pipeline\n",
    "            over = eval('%s(%s)' % (key_1,SMOTE_dict[key_1]))\n",
    "            under = RandomUnderSampler(sampling_strategy=under_sampling_dict, random_state=1)\n",
    "            steps = [('o', over), ('u', under), ('model',clf)]\n",
    "            pipeline = Pipeline(steps=steps)\n",
    "\n",
    "\n",
    "            #evaluate pipeline\n",
    "\n",
    "            # define cross-validation method for model evaluation\n",
    "            cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)       \n",
    "\n",
    "            #calculate cross-validated mean per class accuracy (recall macroaverage from classification report)\n",
    "            results = cross_validate(pipeline, arg1, arg2, scoring=['recall_macro','accuracy','f1_macro','f1_weighted'], cv=cv, n_jobs=-1,error_score='raise')\n",
    "\n",
    "            #append cross-validation results to respective lists\n",
    "            accuracy.append(results['test_accuracy'].mean())\n",
    "            mpca.append(results['test_recall_macro'].mean())\n",
    "            f1_wt.append(results['test_f1_weighted'].mean())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #train each model with training data and predict on test data\n",
    "\n",
    "            # Split the data into training and testing sets\n",
    "            X_train, X_test, y_train, y_test = tts(arg1, arg2, test_size=0.03, random_state=arg4,stratify=arg2)\n",
    "\n",
    "            # Create a dictionary to store the indices of each label in the training set\n",
    "            label_indices = {}\n",
    "            for i in range(len(y_train)):\n",
    "                label = y_train[i]\n",
    "                if label not in label_indices:\n",
    "                    label_indices[label] = []\n",
    "                label_indices[label].append(i)\n",
    "\n",
    "            # Create a list to store the 5 samples from each label\n",
    "            samples = []\n",
    "            for label in label_indices:\n",
    "                label_samples = label_indices[label][:5] # Get the first 5 indices for each label\n",
    "                for idx in label_samples:\n",
    "                    samples.append(idx)\n",
    "\n",
    "            # Get the data and labels for the selected samples\n",
    "            X_test = X_train[samples]\n",
    "            y_test = y_train[samples]\n",
    "\n",
    "\n",
    "            #train models\n",
    "            model = pipeline.fit(X_train, y_train)       \n",
    "\n",
    "            #predict on test data\n",
    "            y_hat = model.predict(X_test)      \n",
    "            print(classification_report(y_test, y_hat))\n",
    "\n",
    "            #print confusion matrix\n",
    "            CM = confusion_matrix(y_test,y_hat)\n",
    "            print(CM)\n",
    "\n",
    "\n",
    "            #capture classification accuracy metrics\n",
    "            report_dict = classification_report(y_test,y_hat,output_dict=True)\n",
    "\n",
    "            #mean per class accuracy\n",
    "            mpca2.append(report_dict['macro avg']['recall']) #mean per class accuracy\n",
    "\n",
    "            #return f1 score\n",
    "            f1_wt2.append(report_dict['weighted avg']['f1-score'])\n",
    "\n",
    "            #accuracy\n",
    "            accuracy2.append(report_dict['accuracy'])   \n",
    "\n",
    "            #misclassification cost\n",
    "            cost2.append(misclassification_cost(y_test,y_hat))\n",
    "\n",
    "            #unique label predictions\n",
    "            unique2.append(np.unique(y_hat))\n",
    "\n",
    "        \n",
    "        \n",
    "        #create and populate dataframe with cross-validation results\n",
    "        df_scores = pd.DataFrame()\n",
    "        df_scores['Model'] = models\n",
    "        df_scores['CV Accuracy'] = accuracy\n",
    "        df_scores['CV MPCA'] = mpca\n",
    "        df_scores['CV F1_weighted'] = f1_wt\n",
    "\n",
    "\n",
    "        #create and populate dataframe with trained model results\n",
    "        df_scores2 = pd.DataFrame()\n",
    "        df_scores2['SMOTE MDL'] = smote\n",
    "        df_scores2['Model'] = models\n",
    "        df_scores2['Accuracy'] = accuracy2\n",
    "        df_scores2['MPCA'] = mpca2\n",
    "        df_scores2['F1_weighted'] = f1_wt2\n",
    "        df_scores2['Misclassification_Cost'] = cost2\n",
    "        df_scores2['Unique Predictions'] = unique2\n",
    "        \n",
    "        #append results to dataframe\n",
    "        output = output.append(df_scores2)\n",
    "        \n",
    "        print(key_1)\n",
    "        \n",
    "        #save dataframe as png\n",
    "        #df_styled = df_scores2.style.background_gradient()\n",
    "        #dfi.export(df_styled,'fullcanopy_mb_%s_EXB.png' % (key_1), table_conversion = 'matplotlib')\n",
    "    \n",
    "    return(df_scores2,model,X_test,y_test,X_train,y_train,df_scores,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define SMOTE undersampling and oversampling rates for each model's dataset\n",
    "'''\n",
    "osr = oversample rate\n",
    "usr = undersample rate\n",
    "usd = undersample dictionary\n",
    "osd = oversample dictionary\n",
    "'''\n",
    "\n",
    "mb_osr = int(0.7*max_count_base)\n",
    "mb_usr = int(0.7*max_count_base)\n",
    "mb_usd = {}\n",
    "mb_osd = {1:mb_osr,2:mb_osr}\n",
    "\n",
    "m0_osr = int(0.7*max_count_m0)\n",
    "m0_usr = int(0.7*max_count_m0)\n",
    "m0_usd = {}\n",
    "m0_osd = {1:int(0.7*max_count_m0)}\n",
    "\n",
    "m1_osr = int(0.85*max_count_m1)\n",
    "m1_usr = int(0.7*max_count_m1)\n",
    "m1_usd = {}\n",
    "m1_osd = {1:m1_osr}\n",
    "\n",
    "m2_osr = int(0.85*max_count_m2)\n",
    "m2_usr = {}\n",
    "m2_usd = {}\n",
    "m2_osd = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Varying random seed when splitting test and training data to assess variance in classifier accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVMSMOTE\n",
      "RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         5\n",
      "           2       1.00      1.00      1.00         5\n",
      "           3       1.00      1.00      1.00         5\n",
      "           4       1.00      1.00      1.00         5\n",
      "           5       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00        25\n",
      "   macro avg       1.00      1.00      1.00        25\n",
      "weighted avg       1.00      1.00      1.00        25\n",
      "\n",
      "[[5 0 0 0 0]\n",
      " [0 5 0 0 0]\n",
      " [0 0 5 0 0]\n",
      " [0 0 0 5 0]\n",
      " [0 0 0 0 5]]\n",
      "[[5 0 0 0 0]\n",
      " [0 5 0 0 0]\n",
      " [0 0 5 0 0]\n",
      " [0 0 0 5 0]\n",
      " [0 0 0 0 5]]\n",
      "SVMSMOTE\n",
      "SVMSMOTE\n",
      "RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         5\n",
      "           2       1.00      1.00      1.00         5\n",
      "           3       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00        15\n",
      "   macro avg       1.00      1.00      1.00        15\n",
      "weighted avg       1.00      1.00      1.00        15\n",
      "\n",
      "[[5 0 0]\n",
      " [0 5 0]\n",
      " [0 0 5]]\n",
      "[[5 0 0]\n",
      " [0 5 0]\n",
      " [0 0 5]]\n",
      "SVMSMOTE\n",
      "SVMSMOTE\n",
      "RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         5\n",
      "           2       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00        10\n",
      "   macro avg       1.00      1.00      1.00        10\n",
      "weighted avg       1.00      1.00      1.00        10\n",
      "\n",
      "[[5 0]\n",
      " [0 5]]\n",
      "[[5 0]\n",
      " [0 5]]\n",
      "SVMSMOTE\n",
      "SVMSMOTE\n",
      "RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       1.00      1.00      1.00         5\n",
      "           5       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00        10\n",
      "   macro avg       1.00      1.00      1.00        10\n",
      "weighted avg       1.00      1.00      1.00        10\n",
      "\n",
      "[[5 0]\n",
      " [0 5]]\n",
      "[[5 0]\n",
      " [0 5]]\n",
      "SVMSMOTE\n",
      "SVMSMOTE\n",
      "RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         5\n",
      "           2       1.00      1.00      1.00         5\n",
      "           3       1.00      1.00      1.00         5\n",
      "           4       1.00      1.00      1.00         5\n",
      "           5       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00        25\n",
      "   macro avg       1.00      1.00      1.00        25\n",
      "weighted avg       1.00      1.00      1.00        25\n",
      "\n",
      "[[5 0 0 0 0]\n",
      " [0 5 0 0 0]\n",
      " [0 0 5 0 0]\n",
      " [0 0 0 5 0]\n",
      " [0 0 0 0 5]]\n",
      "[[5 0 0 0 0]\n",
      " [0 5 0 0 0]\n",
      " [0 0 5 0 0]\n",
      " [0 0 0 5 0]\n",
      " [0 0 0 0 5]]\n",
      "SVMSMOTE\n",
      "SVMSMOTE\n",
      "RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         5\n",
      "           2       1.00      1.00      1.00         5\n",
      "           3       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00        15\n",
      "   macro avg       1.00      1.00      1.00        15\n",
      "weighted avg       1.00      1.00      1.00        15\n",
      "\n",
      "[[5 0 0]\n",
      " [0 5 0]\n",
      " [0 0 5]]\n",
      "[[5 0 0]\n",
      " [0 5 0]\n",
      " [0 0 5]]\n",
      "SVMSMOTE\n",
      "SVMSMOTE\n",
      "RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         5\n",
      "           2       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00        10\n",
      "   macro avg       1.00      1.00      1.00        10\n",
      "weighted avg       1.00      1.00      1.00        10\n",
      "\n",
      "[[5 0]\n",
      " [0 5]]\n",
      "[[5 0]\n",
      " [0 5]]\n",
      "SVMSMOTE\n",
      "SVMSMOTE\n",
      "RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       1.00      1.00      1.00         5\n",
      "           5       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00        10\n",
      "   macro avg       1.00      1.00      1.00        10\n",
      "weighted avg       1.00      1.00      1.00        10\n",
      "\n",
      "[[5 0]\n",
      " [0 5]]\n",
      "[[5 0]\n",
      " [0 5]]\n",
      "SVMSMOTE\n",
      "SVMSMOTE\n",
      "RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         5\n",
      "           2       1.00      1.00      1.00         5\n",
      "           3       1.00      1.00      1.00         5\n",
      "           4       1.00      1.00      1.00         5\n",
      "           5       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00        25\n",
      "   macro avg       1.00      1.00      1.00        25\n",
      "weighted avg       1.00      1.00      1.00        25\n",
      "\n",
      "[[5 0 0 0 0]\n",
      " [0 5 0 0 0]\n",
      " [0 0 5 0 0]\n",
      " [0 0 0 5 0]\n",
      " [0 0 0 0 5]]\n",
      "[[5 0 0 0 0]\n",
      " [0 5 0 0 0]\n",
      " [0 0 5 0 0]\n",
      " [0 0 0 5 0]\n",
      " [0 0 0 0 5]]\n",
      "SVMSMOTE\n",
      "SVMSMOTE\n",
      "RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         5\n",
      "           2       1.00      1.00      1.00         5\n",
      "           3       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00        15\n",
      "   macro avg       1.00      1.00      1.00        15\n",
      "weighted avg       1.00      1.00      1.00        15\n",
      "\n",
      "[[5 0 0]\n",
      " [0 5 0]\n",
      " [0 0 5]]\n",
      "[[5 0 0]\n",
      " [0 5 0]\n",
      " [0 0 5]]\n",
      "SVMSMOTE\n",
      "SVMSMOTE\n",
      "RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         5\n",
      "           2       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00        10\n",
      "   macro avg       1.00      1.00      1.00        10\n",
      "weighted avg       1.00      1.00      1.00        10\n",
      "\n",
      "[[5 0]\n",
      " [0 5]]\n",
      "[[5 0]\n",
      " [0 5]]\n",
      "SVMSMOTE\n",
      "SVMSMOTE\n",
      "RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       1.00      1.00      1.00         5\n",
      "           5       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00        10\n",
      "   macro avg       1.00      1.00      1.00        10\n",
      "weighted avg       1.00      1.00      1.00        10\n",
      "\n",
      "[[5 0]\n",
      " [0 5]]\n",
      "[[5 0]\n",
      " [0 5]]\n",
      "SVMSMOTE\n",
      "SVMSMOTE\n",
      "RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         5\n",
      "           2       1.00      1.00      1.00         5\n",
      "           3       1.00      1.00      1.00         5\n",
      "           4       1.00      1.00      1.00         5\n",
      "           5       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00        25\n",
      "   macro avg       1.00      1.00      1.00        25\n",
      "weighted avg       1.00      1.00      1.00        25\n",
      "\n",
      "[[5 0 0 0 0]\n",
      " [0 5 0 0 0]\n",
      " [0 0 5 0 0]\n",
      " [0 0 0 5 0]\n",
      " [0 0 0 0 5]]\n",
      "[[5 0 0 0 0]\n",
      " [0 5 0 0 0]\n",
      " [0 0 5 0 0]\n",
      " [0 0 0 5 0]\n",
      " [0 0 0 0 5]]\n",
      "SVMSMOTE\n",
      "SVMSMOTE\n",
      "RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         5\n",
      "           2       1.00      1.00      1.00         5\n",
      "           3       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00        15\n",
      "   macro avg       1.00      1.00      1.00        15\n",
      "weighted avg       1.00      1.00      1.00        15\n",
      "\n",
      "[[5 0 0]\n",
      " [0 5 0]\n",
      " [0 0 5]]\n",
      "[[5 0 0]\n",
      " [0 5 0]\n",
      " [0 0 5]]\n",
      "SVMSMOTE\n",
      "SVMSMOTE\n",
      "RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         5\n",
      "           2       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00        10\n",
      "   macro avg       1.00      1.00      1.00        10\n",
      "weighted avg       1.00      1.00      1.00        10\n",
      "\n",
      "[[5 0]\n",
      " [0 5]]\n",
      "[[5 0]\n",
      " [0 5]]\n",
      "SVMSMOTE\n",
      "SVMSMOTE\n",
      "RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       1.00      1.00      1.00         5\n",
      "           5       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00        10\n",
      "   macro avg       1.00      1.00      1.00        10\n",
      "weighted avg       1.00      1.00      1.00        10\n",
      "\n",
      "[[5 0]\n",
      " [0 5]]\n",
      "[[5 0]\n",
      " [0 5]]\n",
      "SVMSMOTE\n",
      "SVMSMOTE\n",
      "RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         5\n",
      "           2       1.00      1.00      1.00         5\n",
      "           3       1.00      1.00      1.00         5\n",
      "           4       1.00      1.00      1.00         5\n",
      "           5       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00        25\n",
      "   macro avg       1.00      1.00      1.00        25\n",
      "weighted avg       1.00      1.00      1.00        25\n",
      "\n",
      "[[5 0 0 0 0]\n",
      " [0 5 0 0 0]\n",
      " [0 0 5 0 0]\n",
      " [0 0 0 5 0]\n",
      " [0 0 0 0 5]]\n",
      "[[5 0 0 0 0]\n",
      " [0 5 0 0 0]\n",
      " [0 0 5 0 0]\n",
      " [0 0 0 5 0]\n",
      " [0 0 0 0 5]]\n",
      "SVMSMOTE\n",
      "SVMSMOTE\n",
      "RandomForestClassifier\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         5\n",
      "           2       1.00      1.00      1.00         5\n",
      "           3       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00        15\n",
      "   macro avg       1.00      1.00      1.00        15\n",
      "weighted avg       1.00      1.00      1.00        15\n",
      "\n",
      "[[5 0 0]\n",
      " [0 5 0]\n",
      " [0 0 5]]\n",
      "[[5 0 0]\n",
      " [0 5 0]\n",
      " [0 0 5]]\n",
      "SVMSMOTE\n",
      "SVMSMOTE\n",
      "RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         5\n",
      "           2       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00        10\n",
      "   macro avg       1.00      1.00      1.00        10\n",
      "weighted avg       1.00      1.00      1.00        10\n",
      "\n",
      "[[5 0]\n",
      " [0 5]]\n",
      "[[5 0]\n",
      " [0 5]]\n",
      "SVMSMOTE\n",
      "SVMSMOTE\n",
      "RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       1.00      1.00      1.00         5\n",
      "           5       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00        10\n",
      "   macro avg       1.00      1.00      1.00        10\n",
      "weighted avg       1.00      1.00      1.00        10\n",
      "\n",
      "[[5 0]\n",
      " [0 5]]\n",
      "[[5 0]\n",
      " [0 5]]\n",
      "SVMSMOTE\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "# Varying random seed when splitting test and training data to assess variance in classifier accuracy\n",
    "rdm_seed_lst = [42,43,44,45,46]\n",
    "\n",
    "#define arguments for the classification model for each dataset\n",
    "mb_args = [X,y,max_count_base,rdm_seed_lst[i],mb_osr,mb_usr,mb_usd,mb_osd]\n",
    "m0_args = [X_m0,y_m0,max_count_m0,rdm_seed_lst[i],m0_osr,m0_usr,m0_usd,m0_osd]\n",
    "m1_args = [X_m1,y_m1,max_count_m1,rdm_seed_lst[i],m1_osr,m1_usr,m1_usd,m1_osd]\n",
    "m2_args = [X_m2,y_m2,max_count_m2,rdm_seed_lst[i],m2_osr,m2_usr,m2_usd,m2_osd]\n",
    "\n",
    "# train model on each dataset using the arguments defined above\n",
    "\n",
    "output_mb = pd.DataFrame()\n",
    "output_m0 = pd.DataFrame()\n",
    "output_m1 = pd.DataFrame()\n",
    "output_m2 = pd.DataFrame()\n",
    "\n",
    "\n",
    "for i in range(len(rdm_seed_lst)): \n",
    "    mb_results = classification_model(*mb_args)\n",
    "    m0_results = classification_model(*m0_args)\n",
    "    m1_results = classification_model(*m1_args)\n",
    "    m2_results = classification_model(*m2_args)\n",
    "    \n",
    "    output_mb = output_mb.append(mb_results[7])\n",
    "    output_m0 = output_m0.append(m0_results[7])\n",
    "    output_m1 = output_m1.append(m1_results[7])\n",
    "    output_m2 = output_m2.append(m2_results[7])\n",
    "    \n",
    "#output.to_excel('mb_output_2021colors_EX1B.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical classification with trained models (M0, M1, and M2)\n",
    "\n",
    "### <u>Logic</u>\n",
    "1. Use Model M0 to predict the bin (1, 2, or 3) the sample belongs to.\n",
    "2. If the predicted bin is 1, use Model M1 to predict an IDC rating of 1 or 2.\n",
    "3. Else if Model M0 predicts the bin to be 2, return an IDC Rating of 3.\n",
    "4. Else if Model M0 predicts the bin to be 3, use Model M2 to predict and IDC Rating of 4 or 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        21\n",
      "           2       1.00      1.00      1.00        31\n",
      "           3       1.00      0.97      0.98        31\n",
      "           4       0.98      0.98      0.98        41\n",
      "           5       0.98      1.00      0.99        57\n",
      "\n",
      "    accuracy                           0.99       181\n",
      "   macro avg       0.99      0.99      0.99       181\n",
      "weighted avg       0.99      0.99      0.99       181\n",
      "\n",
      "[[21  0  0  0  0]\n",
      " [ 0 31  0  0  0]\n",
      " [ 0  0 30  1  0]\n",
      " [ 0  0  0 40  1]\n",
      " [ 0  0  0  0 57]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        21\n",
      "           2       1.00      1.00      1.00        31\n",
      "           3       1.00      0.97      0.98        31\n",
      "           4       0.98      0.98      0.98        41\n",
      "           5       0.98      1.00      0.99        57\n",
      "\n",
      "    accuracy                           0.99       181\n",
      "   macro avg       0.99      0.99      0.99       181\n",
      "weighted avg       0.99      0.99      0.99       181\n",
      "\n",
      "[[21  0  0  0  0]\n",
      " [ 0 31  0  0  0]\n",
      " [ 0  0 30  1  0]\n",
      " [ 0  0  0 40  1]\n",
      " [ 0  0  0  0 57]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        21\n",
      "           2       1.00      1.00      1.00        31\n",
      "           3       1.00      1.00      1.00        31\n",
      "           4       1.00      1.00      1.00        41\n",
      "           5       1.00      1.00      1.00        57\n",
      "\n",
      "    accuracy                           1.00       181\n",
      "   macro avg       1.00      1.00      1.00       181\n",
      "weighted avg       1.00      1.00      1.00       181\n",
      "\n",
      "[[21  0  0  0  0]\n",
      " [ 0 31  0  0  0]\n",
      " [ 0  0 31  0  0]\n",
      " [ 0  0  0 41  0]\n",
      " [ 0  0  0  0 57]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        21\n",
      "           2       1.00      1.00      1.00        31\n",
      "           3       1.00      1.00      1.00        31\n",
      "           4       1.00      1.00      1.00        41\n",
      "           5       1.00      1.00      1.00        57\n",
      "\n",
      "    accuracy                           1.00       181\n",
      "   macro avg       1.00      1.00      1.00       181\n",
      "weighted avg       1.00      1.00      1.00       181\n",
      "\n",
      "[[21  0  0  0  0]\n",
      " [ 0 31  0  0  0]\n",
      " [ 0  0 31  0  0]\n",
      " [ 0  0  0 41  0]\n",
      " [ 0  0  0  0 57]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        21\n",
      "           2       1.00      1.00      1.00        31\n",
      "           3       1.00      1.00      1.00        31\n",
      "           4       1.00      1.00      1.00        41\n",
      "           5       1.00      1.00      1.00        57\n",
      "\n",
      "    accuracy                           1.00       181\n",
      "   macro avg       1.00      1.00      1.00       181\n",
      "weighted avg       1.00      1.00      1.00       181\n",
      "\n",
      "[[21  0  0  0  0]\n",
      " [ 0 31  0  0  0]\n",
      " [ 0  0 31  0  0]\n",
      " [ 0  0  0 41  0]\n",
      " [ 0  0  0  0 57]]\n"
     ]
    }
   ],
   "source": [
    "#hierarchical classification of validation data\n",
    "\n",
    "rdm_seed_lst = [42,43,44,45,46]\n",
    "\n",
    "\n",
    "seed = []\n",
    "mpca = []\n",
    "f1_wt = []\n",
    "cost = []\n",
    "accuracy = []\n",
    "unique_predictions = []\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(rdm_seed_lst)):\n",
    "    seed.append(rdm_seed_lst[i])\n",
    "    \n",
    "    #mb = smote_classifier_base(X,y,max_count_base,rdm_seed_lst[i\n",
    "    X_train, X_test, y_train, y_test = tts(X,y, random_state=i, stratify = y)\n",
    "    mbx = [X_train,  y_train, X_test, y_test]\n",
    "\n",
    "    pred_list = []\n",
    "    for i in range(0,len(mbx[2])):\n",
    "        if m0_results[1].predict(mbx[2])[i] == 1:    #idc score is 1 or 2\n",
    "            pred = m1_results[1].predict(mbx[2])[i]  #use model 1 to classify as 1 or 2\n",
    "            pred_list.append(pred)                          \n",
    "        elif m0_results[1].predict(mbx[2])[i] == 2:\n",
    "            pred = 3\n",
    "            pred_list.append(pred)\n",
    "        elif m0_results[1].predict(mbx[2])[i] == 3:   #idc score is 4 or 5\n",
    "            pred = m2_results[1].predict(mbx[2])[i]   #use model 2 to classify as 4 or 5\n",
    "            pred_list.append(pred)\n",
    "    pred_list = np.array(pred_list)                         \n",
    "\n",
    "    print(classification_report(mbx[3],pred_list))\n",
    "    \n",
    "    #print classification report\n",
    "    report_dict = classification_report(mbx[3],pred_list,output_dict=True)\n",
    "\n",
    "    #mean per-class accuracy\n",
    "    mpca.append(report_dict['macro avg']['recall']) #mean per class accuracy\n",
    "\n",
    "    #return f1 score\n",
    "    f1_wt.append(report_dict['weighted avg']['f1-score'])\n",
    "\n",
    "    #accuracy\n",
    "    accuracy.append(accuracy_score(pred_list,mbx[3]))       \n",
    "\n",
    "    #uniqueness of the predictions\n",
    "    unique_predictions.append(np.unique(pred_list))\n",
    "\n",
    "    #misclassification cost\n",
    "    cost.append(misclassification_cost(mbx[3],pred_list))\n",
    "    \n",
    "df_hierarchy_cv = pd.DataFrame()\n",
    "df_hierarchy_cv['Seed'] = seed\n",
    "df_hierarchy_cv['Accuracy'] = accuracy\n",
    "df_hierarchy_cv['MPCA'] = mpca\n",
    "df_hierarchy_cv['F1_weighted'] = f1_wt\n",
    "df_hierarchy_cv['Misclassification Cost'] = cost\n",
    "\n",
    "#df_hierarchy_cv.to_excel('hierarchy_results.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Seed</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>MPCA</th>\n",
       "      <th>F1_weighted</th>\n",
       "      <th>Misclassification Cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42</td>\n",
       "      <td>0.98895</td>\n",
       "      <td>0.98867</td>\n",
       "      <td>0.988929</td>\n",
       "      <td>0.01105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43</td>\n",
       "      <td>0.98895</td>\n",
       "      <td>0.98867</td>\n",
       "      <td>0.988929</td>\n",
       "      <td>0.01105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Seed  Accuracy     MPCA  F1_weighted  Misclassification Cost\n",
       "0    42   0.98895  0.98867     0.988929                 0.01105\n",
       "1    43   0.98895  0.98867     0.988929                 0.01105\n",
       "2    44   1.00000  1.00000     1.000000                 0.00000\n",
       "3    45   1.00000  1.00000     1.000000                 0.00000\n",
       "4    46   1.00000  1.00000     1.000000                 0.00000"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hierarchy_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Seed</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>MPCA</th>\n",
       "      <th>F1_weighted</th>\n",
       "      <th>Misclassification Cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42</td>\n",
       "      <td>0.928177</td>\n",
       "      <td>0.929883</td>\n",
       "      <td>0.927984</td>\n",
       "      <td>0.071823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43</td>\n",
       "      <td>0.906077</td>\n",
       "      <td>0.904280</td>\n",
       "      <td>0.905217</td>\n",
       "      <td>0.093923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44</td>\n",
       "      <td>0.939227</td>\n",
       "      <td>0.935327</td>\n",
       "      <td>0.939076</td>\n",
       "      <td>0.060773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>0.895028</td>\n",
       "      <td>0.895894</td>\n",
       "      <td>0.891895</td>\n",
       "      <td>0.110497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46</td>\n",
       "      <td>0.917127</td>\n",
       "      <td>0.914036</td>\n",
       "      <td>0.915984</td>\n",
       "      <td>0.082873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Seed  Accuracy      MPCA  F1_weighted  Misclassification Cost\n",
       "0    42  0.928177  0.929883     0.927984                0.071823\n",
       "1    43  0.906077  0.904280     0.905217                0.093923\n",
       "2    44  0.939227  0.935327     0.939076                0.060773\n",
       "3    45  0.895028  0.895894     0.891895                0.110497\n",
       "4    46  0.917127  0.914036     0.915984                0.082873"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hierarchy_cv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
